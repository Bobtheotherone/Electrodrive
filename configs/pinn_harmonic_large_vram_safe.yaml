exp_name: pinn_harmonic_large_vram_safe
seed: 42
device: cuda
train_dtype: bfloat16        # can still be overridden via CLI

dataset:
  n_train_problems: 2048
  n_val_problems: 128
  samples_per_problem_jit: 2048   # shrink per-problem sampling to cut host+device load
  ratio_boundary: 0.4
  ratio_interior: 0.6
  supervision_mode: analytic

model:
  model_type: pinn_harmonic_large
  params:
    width: 512
    depth: 6
    use_fourier: true
    fourier_scale: 10.0

trainer:
  max_epochs: 20
  # DataLoader collates exactly this many points (4 problems × 2048 pts).
  batch_size: 8192
  # Microbatch through the second-order autograd path (kept on device).
  points_per_step: 1024        # start very safe; raise to 2048 later if stable
  # Keep effective batch size high without raising peak VRAM.
  accum_steps: 64              # 1024 × 64 = 65,536 effective points/step
  learning_rate: 5.0e-4
  lr_scheduler: cosine
  weight_decay: 1.0e-5
  grad_clip_norm: 1.0
  amp: bf16                    # train.py understands "bf16"/"fp16"/bool/None
  compile: true
  compile_mode: reduce-overhead
  log_every_n_steps: 1
  val_every_n_epochs: 1
  ckpt_every_n_epochs: 2