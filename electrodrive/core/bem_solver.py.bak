from __future__ import annotations

import math
import time
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

try:
    import torch
    from torch import Tensor
except Exception as e:  # pragma: no cover
    raise ImportError("Torch is required by bem_solver.py (GMRES).") from e

from electrodrive.utils.logging import JsonlLogger
from electrodrive.utils.telemetry import TelemetryWriter

__all__ = ["gmres_restart", "setup_preconditioner", "SOLVER_TRACE_HEADERS"]

SOLVER_TRACE_HEADERS = [
    "ts",
    "pass_idx",
    "cycle",
    "iter",
    "resid_true_l2",
    "resid_precond_l2",
    "arnoldi_dim",
    "phase",
]


def setup_preconditioner(
    method: Optional[str],
    A_diag: Optional[Tensor] = None,
    areas: Optional[Tensor] = None,
    logger: Optional[JsonlLogger] = None,
) -> Optional[Callable[[Tensor], Tensor]]:
    """
    Returns a right-preconditioner apply function M(x), or None.
    Supported: "diag"/"jacobi" (uses A_diag), "mass" (uses areas), "none"/None.
    """
    if method is None or method == "none":
        return None
    if method in ("diag", "jacobi"):
        if A_diag is None:
            if logger:
                logger.warning("Diagonal preconditioner requested but A_diag is None; falling back to 'mass' if areas provided.")
            # Fallback: if areas supplied, use mass preconditioner
            if areas is None:
                return None
            method = "mass"  # fall through to mass
        else:
            finite = torch.isfinite(A_diag)
            if not torch.any(finite):
                if logger:
                    logger.warning("Diagonal preconditioner: no finite entries.")
                return None
            P_inv = 1.0 / A_diag.clamp_min(1e-18)

            def apply(x: Tensor) -> Tensor:
                return P_inv * x

            return apply
    if method == "mass":
        if areas is None:
            if logger:
                logger.warning("Mass preconditioner requested but areas is None.")
            return None
        finite = torch.isfinite(areas)
        if not torch.any(finite):
            if logger:
                logger.warning("Mass preconditioner: no finite entries.")
            return None
        P_inv = 1.0 / areas.clamp_min(1e-18)

        def apply(x: Tensor) -> Tensor:
            return P_inv * x

        return apply
    if logger:
        logger.warning("Unknown preconditioner '%s'; using none.", str(method))
    return None


@torch.no_grad()
def gmres_restart(
    A: Callable[[Tensor], Tensor],
    b: Tensor,
    M: Optional[Callable[[Tensor], Tensor]] = None,
    restart: int = 80,
    tol: float = 1e-8,
    maxiter: int = 500,
    logger: Optional[JsonlLogger] = None,
    x0: Optional[Tensor] = None,
    telemetry: Optional[TelemetryWriter] = None,
    pass_index: int = 0,
    live_control_poll_fn: Optional[Callable[[], Dict[str, Any]]] = None,
    log_every: int = 1,
    *,
    # New, backward-compatible options (ignored unless provided)
    callback: Optional[Callable[[int, float, Dict[str, Any]], None]] = None,
    callback_context: Optional[Dict[str, Any]] = None,
    precond: Optional[Union[str, Callable[[Tensor], Tensor]]] = None,
    areas: Optional[Tensor] = None,
) -> Tuple[Tensor, Dict[str, Any]]:
    """
    Restarted GMRES with optional right-preconditioner M and CSV telemetry.
    Converges when ||r_true|| <= max(tol * ||r_true(0)||, tol * 1e-12).

    Added in this patch (non-breaking):
      - callback(iter_idx, resid_norm, ctx) invoked every inner iteration.
      - precond: str or callable. If str in {"jacobi","diag","mass"}, uses setup_preconditioner.
                 If jacobi requested but no A_diag is available, falls back to 'mass' if 'areas' provided.
    """
    if restart <= 0:
        raise ValueError("restart must be positive")
    if maxiter <= 0:
        raise ValueError("maxiter must be positive")

    # Resolve preconditioner if M not provided
    if M is None and precond is not None:
        if isinstance(precond, str):
            M = setup_preconditioner(precond, A_diag=None, areas=areas, logger=logger)
        elif callable(precond):
            M = precond  # direct apply function

    device, dtype = b.device, b.dtype
    x = torch.zeros_like(b) if x0 is None else x0.clone()

    # Initial residuals
    r_true = b - A(x)
    r = M(r_true) if M is not None else r_true
    beta = torch.linalg.norm(r)
    r_norm = float(beta)
    r_true_norm = float(torch.linalg.norm(r_true))
    init_true = r_true_norm if r_true_norm > 1e-16 else 1.0
    tol_abs = max(tol * init_true, tol * 1e-12)

    if logger:
        logger.info(
            "GMRES started.",
            N=int(b.numel()),
            tol=float(tol),
            tol_abs=float(tol_abs),
            restart=int(restart),
            maxiter=int(maxiter),
            preconditioned=bool(M is not None),
            pass_index=int(pass_index),
        )

    if telemetry:
        telemetry.append_row(
            dict(ts=time.time(), pass_idx=pass_index, cycle=0, iter=0,
                 resid_true_l2=r_true_norm, resid_precond_l2=r_norm,
                 arnoldi_dim=0, phase="start")
        )

    if beta.item() == 0.0:
        return x, dict(iters=0, resid=0.0, success=True, code=0)

    total_iters = 0
    max_cycles = max(1, (maxiter + restart - 1) // restart)
    last_poll = time.time()

    for cycle in range(max_cycles):
        # Arnoldi basis and upper-Hessenberg
        V: List[Tensor] = [r / beta]      # v_0
        H = torch.zeros((restart + 1, restart), device=device, dtype=dtype)
        g = torch.zeros(restart + 1, device=device, dtype=dtype)
        g[0] = beta
        givens: List[Tuple[float, float]] = []
        j = 0

        while j < restart and total_iters < maxiter:
            # Live control poll (terminate/pause)
            if live_control_poll_fn is not None and (time.time() - last_poll) >= 5.0:
                try:
                    state = live_control_poll_fn() or {}
                except Exception:
                    state = {}
                last_poll = time.time()
                if state.get("terminate"):
                    if logger:
                        logger.warning("GMRES termination requested by live control.")
                    return x, dict(iters=int(total_iters), resid=float("nan"), success=False, code=2)
                if state.get("pause"):
                    time.sleep(0.2)

            vj = V[j]
            w = A(vj)
            if M is not None:
                w = M(w)
            total_iters += 1

            # Arnoldi orthogonalization
            for i in range(j + 1):
                H[i, j] = torch.dot(V[i], w)
                w = w - H[i, j] * V[i]
            H[j + 1, j] = torch.linalg.norm(w)
            if float(H[j + 1, j]) > 0.0:
                V.append(w / H[j + 1, j])

            # Apply existing Givens
            for i_g, (cs_i, sn_i) in enumerate(givens):
                temp = cs_i * H[i_g, j] + sn_i * H[i_g + 1, j]
                H[i_g + 1, j] = -sn_i * H[i_g, j] + cs_i * H[i_g + 1, j]
                H[i_g, j] = temp

            # New Givens for this column
            denom = torch.sqrt(H[j, j] ** 2 + H[j + 1, j] ** 2)
            if float(denom) < 1e-14:
                cs_j, sn_j = 1.0, 0.0
            else:
                cs_j = float(H[j, j] / denom)
                sn_j = float(H[j + 1, j] / denom)
            givens.append((cs_j, sn_j))

            # Apply to H and g
            H[j, j] = cs_j * H[j, j] + sn_j * H[j + 1, j]
            H[j + 1, j] = 0.0
            g_j1 = -sn_j * g[j]
            g_j = cs_j * g[j]
            g[j] = g_j
            g[j + 1] = g_j1

            r_norm = abs(float(g[j + 1]))

            if logger and (total_iters % max(1, log_every) == 0):
                logger.debug("GMRES progress.", cycle=int(cycle), iters=int(total_iters),
                             resid_precond=float(r_norm), arnoldi_dim=int(j + 1))
            if telemetry and (total_iters % max(1, log_every) == 0):
                telemetry.append_row(
                    dict(ts=time.time(), pass_idx=pass_index, cycle=cycle, iter=total_iters,
                         resid_true_l2=float("nan"), resid_precond_l2=r_norm,
                         arnoldi_dim=int(j + 1), phase="inner")
                )
            if callback is not None:
                try:
                    ctx = dict(callback_context or {})
                    ctx.setdefault("pass_index", int(pass_index))
                    ctx.setdefault("arnoldi_dim", int(j + 1))
                    callback(int(total_iters), float(r_norm), ctx)
                except Exception:
                    # Never let a callback failure kill the solver
                    pass

            if r_norm <= tol_abs:
                break

            j += 1  # next Arnoldi step

        m_use = j
        if m_use == 0:
            if logger:
                logger.warning("GMRES stagnation: no Arnoldi steps.", cycle=int(cycle), iters=int(total_iters))
            return x, dict(iters=int(total_iters), resid=float(r_norm), success=False, code=2)

        # Back-solve R y = g for y (upper triangular R = H[:m,:m])
        R = H[:m_use, :m_use]
        g_top = g[:m_use]
        # Triangular solve; fallback to lstsq for robustness
        try:
            y = torch.linalg.solve_triangular(R, g_top, upper=True)
        except Exception:
            y = torch.linalg.lstsq(R, g_top).solution

        V_m = torch.stack(V[:m_use], dim=1)  # (n, m)
        dx = V_m @ y
        x = x + dx

        r_true = b - A(x)
        r_true_norm = float(torch.linalg.norm(r_true))
        r = M(r_true) if M is not None else r_true
        beta = torch.linalg.norm(r)
        r_norm = float(beta)

        if logger:
            logger.info("GMRES cycle done.", cycle=int(cycle), iters=int(total_iters),
                        resid_precond=r_norm, resid_true=r_true_norm, arnoldi_dim=int(m_use))
        if telemetry:
            telemetry.append_row(
                dict(ts=time.time(), pass_idx=pass_index, cycle=cycle, iter=total_iters,
                     resid_true_l2=r_true_norm, resid_precond_l2=r_norm,
                     arnoldi_dim=int(m_use), phase="cycle_end")
            )

        if r_true_norm <= tol_abs:
            return x, dict(iters=int(total_iters), resid=float(r_true_norm), success=True, code=0)

        if total_iters >= maxiter:
            break

    # did not converge
    if logger:
        logger.warning("GMRES failed to converge within max iterations.",
                       iters=int(total_iters), resid=float(r_norm), resid_true=float(r_true_norm),
                       maxiter=int(maxiter))
    return x, dict(iters=int(total_iters), resid=float(r_norm), success=False, code=1)
