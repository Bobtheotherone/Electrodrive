from __future__ import annotations

import dataclasses
import datetime as _dt
import io
import json
import logging
import math
import os
import platform
import sys
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

try:
    import torch  # type: ignore
except Exception:  # pragma: no cover
    torch = None  # type: ignore


# --------------------------------------------
# JSON utilities (NaN/Inf safe + compact)
# --------------------------------------------

def _json_sanitize(v: Any) -> Any:
    """Convert values into JSON-safe primitives; stringify NaN/Inf."""
    # floats
    try:
        if isinstance(v, float):
            if math.isfinite(v):
                return v
            return "NaN" if math.isnan(v) else ("Infinity" if v > 0 else "-Infinity")
    except Exception:
        pass
    # tensors
    if torch is not None:
        try:
            if isinstance(v, torch.Tensor):
                return v.detach().cpu().tolist()
        except Exception:
            pass
    # basic containers
    if isinstance(v, dict):
        return {str(k): _json_sanitize(val) for k, val in v.items()}
    if isinstance(v, (list, tuple)):
        return [_json_sanitize(x) for x in v]
    if isinstance(v, (set, frozenset)):
        return sorted(_json_sanitize(x) for x in v)
    # other
    try:
        json.dumps(v)
        return v
    except Exception:
        return str(v)


def _json_dump_line(obj: Dict[str, Any]) -> str:
    return json.dumps(_json_sanitize(obj), separators=(",", ":"), ensure_ascii=False)


# --------------------------------------------
# Perf flags (stable API)
# --------------------------------------------

@dataclass(frozen=True)
class RuntimePerfFlags:
    amp: bool = False
    train_dtype: str = "float32"  # for learn stack
    compile: bool = False         # request torch.compile() where available
    tf32: str = "off"             # "off"|"high"|"medium"|"highest"


# --------------------------------------------
# JSONL Logger (append-only, thread-safe)
# --------------------------------------------

class JsonlLogger:
    """
    Minimal, robust JSONL event logger.

    - Safe for NaN/Inf; values are sanitized.
    - Never raises to callers.
    - .info/.debug/.warning/.error all write a single JSON object per line.
    - Adds "ts", "level", "msg" fields plus any structured k/v pairs.
    - Compatible with prior code (same class name/API).
    - Adds SmartLog helpers (health/efficiency/accuracy/logic).
    """

    def __init__(self, out_dir: Path | str):
        self.dir = Path(out_dir)
        self.dir.mkdir(parents=True, exist_ok=True)
        self.path = self.dir / "events.jsonl"
        self._lock = threading.Lock()
        self._stream: Optional[io.TextIOBase] = None
        self._open()

    def _open(self) -> None:
        try:
            self._stream = self.path.open("a", encoding="utf-8")
        except Exception:
            self._stream = None

    def close(self) -> None:
        try:
            if self._stream:
                self._stream.flush()
                self._stream.close()
        except Exception:
            pass

    # ------------- Core write -------------
    def _emit(self, level: str, msg: str, **fields: Any) -> None:
        rec = {
            "ts": _dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "level": level,
            "msg": msg,
        }
        rec.update(fields or {})
        try:
            line = _json_dump_line(rec)
        except Exception:
            # last-resort: stringify everything
            rec2 = {k: str(v) for k, v in rec.items()}
            line = json.dumps(rec2, ensure_ascii=False)
        with self._lock:
            try:
                if self._stream is None:
                    self._open()
                if self._stream:
                    self._stream.write(line + "\n")
                    self._stream.flush()
            except Exception:
                pass

    # ------------- Public API -------------
    def info(self, msg: str, **fields: Any) -> None:
        self._emit("INFO", msg, **fields)

    def debug(self, msg: str, **fields: Any) -> None:
        self._emit("DEBUG", msg, **fields)

    def warning(self, msg: str, **fields: Any) -> None:
        self._emit("WARN", msg, **fields)

    def error(self, msg: str, **fields: Any) -> None:
        # exc_info=True can be passed explicitly by caller to include trace
        if fields.get("exc_info"):
            try:
                import traceback
                fields["trace"] = traceback.format_exc()
            except Exception:
                pass
        self._emit("ERROR", msg, **fields)

    # --------------------------------------------
    # Smart Log mix-in (health/efficiency/accuracy/logic)
    # --------------------------------------------
    def smart_progress(self, **fields: Any) -> None:
        """
        Generic progress probe (loss/residuals/iter/epoch/throughput).
        Use keys like: step, epoch, loss, resid_true_l2, resid_bc, samples_per_sec, etc.
        """
        fields.setdefault("type", "smart_progress")
        self._emit("INFO", "Smart progress", **fields)

    def smart_health(self, **fields: Any) -> None:
        """
        System health snapshot (gpu_mem_alloc_mb, gpu_mem_reserved_mb, tf32_mode, dtype, etc).
        """
        # auto-augment with current VRAM stats if torch+CUDA are up
        if torch is not None and hasattr(torch, "cuda") and torch.cuda.is_available():
            try:
                dev = torch.cuda.current_device()
                fields.setdefault("gpu_mem_alloc_mb", float(torch.cuda.memory_allocated(dev)) / (1024.0 * 1024.0))
                fields.setdefault("gpu_mem_reserved_mb", float(torch.cuda.memory_reserved(dev)) / (1024.0 * 1024.0))
            except Exception:
                pass
        fields.setdefault("type", "smart_health")
        self._emit("INFO", "Smart health", **fields)

    def smart_efficiency(self, **fields: Any) -> None:
        """
        Efficiency probe (tiles_per_sec, batches_per_sec, autotile, occupancy proxies).
        """
        fields.setdefault("type", "smart_efficiency")
        self._emit("DEBUG", "Smart efficiency", **fields)

    def smart_accuracy(self, **fields: Any) -> None:
        """
        Accuracy snapshot (bc_linf, dual_l2, pde_linf, energy_rel_diff).
        """
        fields.setdefault("type", "smart_accuracy")
        self._emit("INFO", "Smart accuracy", **fields)

    def smart_logic(self, **fields: Any) -> None:
        """
        Logical checks (max_principle_margin, reciprocity_dev, invariances).
        """
        fields.setdefault("type", "smart_logic")
        self._emit("INFO", "Smart logic", **fields)

    def phase_start(self, name: str, **fields: Any) -> None:
        self._emit("INFO", "Phase start", phase=name, **fields)

    def phase_end(self, name: str, **fields: Any) -> None:
        self._emit("INFO", "Phase end", phase=name, **fields)


# --------------------------------------------
# TF32 helpers + runtime environment logging
# --------------------------------------------

def get_effective_tf32_mode() -> str:
    if torch is None:
        return "unavailable"
    try:
        if hasattr(torch, "get_float32_matmul_precision"):
            mode = str(torch.get_float32_matmul_precision())
            if mode:
                return mode
    except Exception:
        pass
    try:
        # legacy matmul knobs as fallback
        if hasattr(torch.backends, "cuda") and hasattr(torch.backends.cuda, "matmul"):
            if torch.backends.cuda.matmul.allow_tf32:  # type: ignore[attr-defined]
                return "high"
            return "highest"
    except Exception:
        pass
    return "unknown"


def log_runtime_environment(logger: JsonlLogger, perf_flags: Optional[RuntimePerfFlags] = None) -> None:
    """Best-effort environment summary."""
    v: Dict[str, Any] = {
        "python": sys.version.replace("\n", " "),
        "platform": platform.platform(),
        "executable": sys.executable,
    }
    try:
        import numpy as _np  # type: ignore
        v["numpy"] = _np.__version__
    except Exception:
        v["numpy"] = "unavailable"
    if torch is not None:
        v["torch"] = getattr(torch, "__version__", "unknown")
        try:
            cuda_ok = torch.cuda.is_available()
        except Exception:
            cuda_ok = False
        v["cuda_available"] = bool(cuda_ok)
        if cuda_ok:
            try:
                dev = torch.cuda.current_device()
                props = torch.cuda.get_device_properties(dev)
                v["device_name"] = props.name
                v["total_memory_gb"] = float(props.total_memory) / (1024.0**3)
            except Exception:
                pass
        v["tf32_effective"] = get_effective_tf32_mode()
        try:
            v["default_dtype"] = str(torch.get_default_dtype())
        except Exception:
            pass
    if perf_flags:
        v["perf_flags"] = dataclasses.asdict(perf_flags)
    logger.info("Runtime environment.", **v)


def log_peak_vram(logger: JsonlLogger, phase: str = "run") -> None:
    """Log peak VRAM usage so far; CPU-only if CUDA is absent."""
    rec: Dict[str, Any] = {"phase": phase, "peak_vram_mb": 0.0}
    if torch is not None and hasattr(torch, "cuda"):
        try:
            if torch.cuda.is_available():
                dev = torch.cuda.current_device()
                rec["peak_vram_mb"] = float(torch.cuda.max_memory_allocated(dev)) / (1024.0 * 1024.0)
                rec["peak_vram_reserved_mb"] = float(torch.cuda.max_memory_reserved(dev)) / (1024.0 * 1024.0)
        except Exception:
            pass
    logger.info("VRAM peak usage.", **rec)
